```{r}
if(!require('pacman')) {install.packages('pacman')}
pacman::p_load(ISLR, readxl, tidyverse, magrittr, dplyr, ggplot2, ggrepel, data.table, lubridate, ggpubr, patchwork, car, glmnet, e1071, caret, xgboost, keras, randomForest)
```

```{r}
solarData <- fread("data/SolarFull.csv")
solarData$`Cloud Type` <- as.factor(solarData$`Cloud Type`)

solarData <- (solarData[which(solarData$`Solar Zenith Angle` < 90)])

# summary(solarData)
```

```{r}
N <- length(solarData$GHI) 
n1 <- floor(.8*N)
set.seed(10)
# Split data to three portions of .6, .2 and .2 of data size N
idx_train <- sample(N, n1)
idx_no_train <- (which(! seq(1:N) %in% idx_train))
data.train <- solarData[idx_train,]
data.test <- solarData[idx_no_train,]

names(solarData)
```

```{r}
fit <- lm(GHI ~ ., data.train)
summary(fit)

fitNoLoc <- lm(GHI ~ .-Location, data.train)
summary(fitNoLoc)

fitTemp <- lm(GHI ~ Temperature, data.train)

# data.test$Prediction.noLoc <- predict(fitNoLoc, data.test, type="response")
# data.test <- data.test %>% mutate(sq.resid.noLoc = (Prediction.noLoc - GHI) * (Prediction.noLoc - GHI))
# MSE.noLoc <- mean(data.test$sq.resid.noLoc)
# MSE.noLoc
# 
# 
# data.test$Prediction.withLoc <- predict(fit, data.test, type="response")
# data.test <- data.test %>% mutate(sq.resid.loc = (Prediction.withLoc - GHI) * (Prediction.withLoc - GHI))
# MSE.Loc <- mean(data.test$sq.resid.loc)
# MSE.Loc


# with backwards elimination
Anova(fitNoLoc)
summary(fitNoLoc)
# fitNoLoc <- update(fitNoLoc, .~.-`Wind Direction`)
# fitNoLoc <- update(fitNoLoc, .~.-`Temperature`)
# fitNoLoc <- update(fitNoLoc, .~.-`Hour`)
fitNoLoc.backwards <- update(fitNoLoc, .~.-`Day`)
fitNoLoc.backwards <- update(fitNoLoc, .~.-`Wind Speed`)
fitNoLoc.backwards <- update(fitNoLoc, .~.-`Minute`)

# fitNoLoc <- update(fitNoLoc, .~.-`Dew Point`)
Anova(fitNoLoc.backwards)
summary(fitNoLoc.backwards)
```

```{r}
getStats <- function(fit, testing_data) {
  testing_data$predict <- predict(fit, testing_data, type="response")
  real.mean <- mean(testing_data$GHI)
  testing_data <- testing_data %>% mutate(sq.resid = (predict - GHI) * (predict - GHI), abs.resid <- abs(predict - GHI), diff.mean <- abs(GHI - real.mean))
  RMSE <- sqrt(mean(testing_data$sq.resid))
  MAE <- mean(testing_data$abs.resid)
  RAE <- sum(testing_data$abs.resid) / sum(testing_data$diff.mean)
  
  return(list(RMSE, MAE, RAE))
}

```

```{r}
# getStats(fit, data.test)
# getStats(fitNoLoc, data.test)
# getStats(fitNoLoc.backwards, data.test)
```

```{r}
# plot(fitNoLoc)

# no standardization as the data is likely not in a bell curve, can try to do a gaussian standardization later
# first we remove location
data.processed <- solarData %>% select(-Location)
# then we do a one hot encoding on the cloud type
dummy <- dummyVars(" ~.", data=data.processed)
data.processed <- data.frame(predict(dummy, data.processed))

# cyclical encoding on month, day, hour, minute, and wind direction
data.processed <- data.processed %>% mutate(sinWD= sin(X.Wind.Direction. * pi / 180), cosWD = cos(X.Wind.Direction. * pi / 180)) %>% select(-X.Wind.Direction.)

data.processed <- data.processed %>% mutate(sin.month = sin(2 * pi * Month / 12), cos.month = cos(2 * pi * Month / 12), sin.day = sin(2 * pi * Day / 31), cos.day = cos(2 * pi * Day / 31), sin.hour = sin(2 * pi * Hour / 24), cos.hour = cos(2 * pi * Hour / 24), sin.minute = sin(2 * pi * Minute / 60), cos.minute = cos(2 * pi * Minute / 60)) %>% select(-c(Month, Day, Hour, Minute))

# min max standardization
ghi_col <- data.processed$GHI
data.processed <- data.processed %>% select(-GHI)

processed <- preProcess(data.processed, method=c("range"))
data.processed <- predict(processed, data.processed)

data.processed$GHI <- ghi_col

data.temp <- data.processed %>% select(c(Year, Pressure, X.Cloud.Type.0, sinWD, cosWD, sin.hour, cos.hour, GHI))
data.temp[sample(nrow(data.temp), 10),]
```

```{r}
N <- length(data.processed$GHI) 
# n1 <- floor(.8*N)
n1 <- 100000
n2 <- 100000
set.seed(10)
idx_train <- sample(N, n1)
idx_no_train <- (which(! seq(1:N) %in% idx_train))
idx_test <- sample(idx_no_train, n2)
idx_val <- (which(! idx_no_train %in% idx_test))
data.train <- data.processed[idx_train,]
data.test <- data.processed[idx_test,]
data.val <- data.processed[idx_val,]
```

```{r}
# run SVR
fitSVR <- svm(GHI ~ ., data.train)
```

```{r}
fitLM <- lm(GHI ~ ., data.train)
```

```{r}
train_x <- data.matrix(data.train[,-ncol(data.train)])
train_y <- data.matrix(data.train[,ncol(data.train)])
test_x <- data.matrix(data.test[,-ncol(data.train)])
test_y <- data.matrix(data.test[,ncol(data.train)])

xgb_train <- xgb.DMatrix(data = train_x, label = train_y)
xgb_test <- xgb.DMatrix(data = test_x, label = test_y)

#defining a watchlist
watchlist <- list(train=xgb_train, test=xgb_test)

#fit XGBoost model and display training and testing data at each iteartion
# max depth = 3 best is around 0.0503 at around 500 epochs
# 5000]	train-rmse:30.037714	test-rmse:65.475067 
# 0.0687 after 2000 epochs at max depth of 2
# 0.067618 at max depth of 3
# 0.0670 at max depth of 4 (round about 300ish epochs)
# 0.06697 at max depth of 5 (at 125 epochs) - best returns
# XGBCV <- xgb.cv(data = xgb_train, max.depth = 11, nrounds = 5000, nfold=2, early_stopping_rounds=20)
# View(XGBCV$evaluation_log)
# 65.41918
# 6 = 65.768
# 7 = 65.022
# 8 = 63.9888 (507)
# 9 = 63.46 (549)
# 10 = 63.16 (398)
# 11 = 62.785 (361)


# fitXGB <- xgb.train(data = xgb_train, max.depth = 4, nrounds = 1311, watchlist=watchlist)
fitXGB <- xgb.train(data = xgb_train, max.depth = 11, nrounds = 361, watchlist=watchlist)

# only showing after the first 10 iterations

fitXGB$evaluation_log[10:nrow(fitXGB$evaluation_log),] %>% ggplot(aes(x=iter)) + geom_point(aes(y=test_rmse), color="red") + geom_point(aes(y=train_rmse), color="blue") + labs(y="RMSE")

predictions <- predict(fitXGB, xgb_test)
RMSE(test_y, predictions)

importance <- xgb.importance(colnames(xgb_train), model=fitXGB)
xgb.plot.importance(importance)
# xgb.save(fitXGB, "XGB300K")

```

```{r}
set.seed(10)
p <- dim(train_x)[2] # number of input variables
fitNN <- keras_model_sequential() %>%
  layer_dense(units = 64, activation = "relu", input_shape = c(p)) %>% 
  # 1 layer with 16 neurons. default activation is relu
  layer_dense(units = 64, activation = "relu") %>%  
  # layer 2 with 8 neurons
  layer_dense(units = 1) # output

fitNN %>% compile(
  optimizer = "Adam",
  loss = "MeanSquaredError",
)

history <- fitNN %>% fit(
  train_x,
  train_y,
  epochs = 15,
  batch_size = 512,
  validation_split = .15 # set 10% of the data3_xtain, data3_ytrain as the validation data
)
```

```{r}
set.seed(10)
fitRF <- randomForest(GHI ~ ., data=data.train, ntree=256)
```

```{r}
getStats(fitSVR, data.test)
getStats(fitLM, data.test)
getStats(fitRF, data.test)
```
```{r}
predictions <- predict(fitXGB, xgb_test)

RMSE(test_y, predictions)
```

```{r}
plot(history)
predictions <- predict(fitNN, test_x)
RMSE(test_y, predictions)
```